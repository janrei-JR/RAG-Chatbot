# RAG System Configuration
# TODO #3 FIX: Embedding Provider Configuration aus Code in YAML verschoben

# =============================================================================
# EMBEDDING PROVIDER CONFIGURATION (TODO #3 FIX)
# =============================================================================
embedding:
  provider_config:
    provider: ollama              # Provider: ollama, openai, local
    model: nomic-embed-text       # Modell-Name
    base_url: http://localhost:11434  # Ollama Server URL
    timeout: 30                   # Request-Timeout in Sekunden (5-300)
    dimension: 768                # Embedding-Dimension (768, 1024, 1536)
    max_retries: 3                # Anzahl Wiederholungsversuche (1-10)
    
  # Fallback Configuration
  fallback:
    enabled: true                 # Fallback aktivieren
    provider: local               # Fallback-Provider
    model: all-MiniLM-L6-v2       # Fallback-Modell

# =============================================================================
# VECTOR STORE CONFIGURATION
# =============================================================================
vectorstore:
  provider: chroma                # chroma, pinecone, faiss
  persist_directory: ./data/vectorstore
  collection_name: rag_documents
  distance_metric: cosine         # cosine, l2, ip

# =============================================================================
# DOCUMENT PROCESSING
# =============================================================================
documents:
  chunk_size: 512                 # Chunk-Größe in Tokens (256-2048)
  chunk_overlap: 50               # Overlap zwischen Chunks (0-200)
  supported_formats:
    - pdf
    - txt
    - md
    - docx
  max_file_size_mb: 10            # Max. Dateigröße in MB

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================
retrieval:
  top_k: 5                        # Anzahl zurückgegebener Dokumente (1-20)
  score_threshold: 0.7            # Min. Similarity-Score (0.0-1.0)
  rerank: true                    # Re-Ranking aktivieren
  hybrid_search: true             # Hybrid-Suche (semantic + keyword)

# =============================================================================
# CHAT CONFIGURATION
# =============================================================================
chat:
  model: llama2                   # LLM-Modell
  temperature: 0.7                # Kreativität (0.0-2.0)
  max_tokens: 2048                # Max. Response-Länge
  context_window: 4096            # Context-Window-Größe

# =============================================================================
# LOGGING
# =============================================================================
logging:
  level: INFO                     # DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: logs/rag_system.log       # Log-Datei-Pfad
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  max_file_size_mb: 10
  backup_count: 5

# =============================================================================
# PERFORMANCE
# =============================================================================
performance:
  batch_size: 32                  # Batch-Größe für Processing
  cache_enabled: true             # Caching aktivieren
  parallel_processing: true       # Parallele Verarbeitung
  max_workers: 4                  # Anzahl Worker-Threads

# =============================================================================
# SECURITY
# =============================================================================
security:
  api_key_required: false         # API-Key-Validierung
  rate_limit_requests: 100        # Requests pro Minute
  cors_enabled: true              # CORS aktivieren